import json
import requests
import cv2
import os
import time
import sys
import subprocess
import datetime
import glob
import argparse
import traceback
import re
import shutil
import hashlib

# ================= è¨­å®šã‚¨ãƒªã‚¢ =================
COMFYUI_URL = "http://127.0.0.1:8188"
DEFAULT_WORKFLOW_FILE = "workflow_api.json"
CHUNK_SIZE = 500           # ãƒ¡ãƒ¢ãƒªä¸è¶³å¯¾ç­–ã§500ãƒ•ãƒ¬ãƒ¼ãƒ åŒºåˆ‡ã‚Š
MAX_PARALLEL_WORKERS = 1   
OUTPUT_EXT = ".mp4"
NODE_ID_LOADER = "1"       
NODE_ID_SAVER = "4"        
TARGET_FPS = 30.0          # â˜…30fpså¼·åˆ¶ (éŸ³ã‚ºãƒ¬é˜²æ­¢ã®è¦)
# ============================================

USER_HOME = os.path.expanduser("~")
COMFYUI_OUTPUT_DIR = os.path.join(USER_HOME, "ComfyUI", "output")
sys.stdout.reconfigure(encoding='utf-8')

def get_video_duration(file_path):
    """å‹•ç”»ã®æ­£ç¢ºãªé•·ã•ï¼ˆç§’ï¼‰ã‚’å–å¾—"""
    try:
        cmd = [
            "ffprobe", "-v", "error", 
            "-show_entries", "format=duration", 
            "-of", "default=noprint_wrappers=1:nokey=1", 
            file_path
        ]
        result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        return float(result.stdout.strip())
    except:
        return 0.0

def queue_prompt(workflow):
    p = {"prompt": workflow}
    data = json.dumps(p).encode('utf-8')
    try:
        resp = requests.post(f"{COMFYUI_URL}/prompt", data=data)
        resp.raise_for_status()
        return resp.json()
    except Exception as e:
        print(f"\n[Fatal Error] Failed to queue prompt: {e}")
        return None

def wait_for_prompt_completion(prompt_id):
    while True:
        try:
            resp = requests.get(f"{COMFYUI_URL}/history/{prompt_id}")
            if resp.status_code == 200:
                if prompt_id in resp.json():
                    return True
        except: pass
        time.sleep(1.0)

def merge_videos_in_folder_smart(target_folder, output_filename, original_video_path):
    print(f"\n=== Merging files inside folder: {os.path.basename(target_folder)} ===")
    
    # ä½œæ¥­ãƒ•ã‚©ãƒ«ãƒ€å†…ã®å…¨MP4ã‚’å–å¾—
    search_pattern = os.path.join(target_folder, f"*{OUTPUT_EXT}")
    all_files = glob.glob(search_pattern)
    
    if not all_files:
        print("âŒ No part files found in the folder.")
        return

    # === é‡è¤‡æ’é™¤ & é¸åˆ¥ãƒ­ã‚¸ãƒƒã‚¯ ===
    part_map = {}
    pattern = re.compile(r"part_(\d+)")

    for f_path in all_files:
        fname = os.path.basename(f_path)
        if "merged" in fname: continue # çµåˆæ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚‚ã—ã‚ã‚Œã°é™¤å¤–
        
        match = pattern.search(fname)
        if match:
            part_idx = int(match.group(1))
            if part_idx not in part_map: part_map[part_idx] = []
            part_map[part_idx].append(f_path)
    
    final_list = []
    sorted_indices = sorted(part_map.keys())
    
    print(f"   Found {len(sorted_indices)} unique parts to merge.")

    for idx in sorted_indices:
        candidates = part_map[idx]
        
        # å€™è£œãŒè¤‡æ•°ã‚ã‚‹å ´åˆï¼ˆä¾‹: part_001.mp4, part_001_audio.mp4ï¼‰
        # åå‰ãŒä¸€ç•ªçŸ­ã„ã‚‚ã®ï¼ˆä½™è¨ˆãªsuffixãŒãªã„ã‚‚ã®ï¼‰ã‚’æ­£ã¨ã™ã‚‹
        if len(candidates) > 1:
            candidates.sort(key=len) 
            selected = candidates[0]
        else:
            selected = candidates[0]
            
        final_list.append(selected)

    # çµåˆç”¨ãƒªã‚¹ãƒˆä½œæˆ
    list_txt = os.path.join(target_folder, "concat_list.txt")
    with open(list_txt, "w", encoding="utf-8") as f:
        for vid in final_list:
            safe_vid = os.path.abspath(vid).replace("'", "'\\''")
            f.write(f"file '{safe_vid}'\n")

    # çµåˆå®Ÿè¡Œ (å…ƒå‹•ç”»ã®éŸ³å£°ã‚’ä½¿ç”¨)
    cmd_final = [
        "ffmpeg", "-y",
        "-f", "concat", "-safe", "0", "-i", list_txt, 
        "-i", original_video_path,        # éŸ³å£°ã‚½ãƒ¼ã‚¹
        "-map", "0:v",                    # æ˜ åƒï¼šçµåˆã—ãŸãƒ‘ãƒ¼ãƒ„
        "-map", "1:a?",                   # éŸ³å£°ï¼šå…ƒå‹•ç”»
        "-c:v", "libx264",                # å†ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        "-preset", "p5",            
        "-crf", "18",
        "-c:a", "aac",              
        output_filename
    ]
    
    try:
        subprocess.run(["ffmpeg", "-hide_banner", "-encoders"], stdout=subprocess.PIPE, stderr=subprocess.DEVNULL)
        cmd_final[cmd_final.index("-c:v") + 1] = "h264_nvenc" # NVENCãŒã‚ã‚Œã°ä½¿ã†
    except: pass 

    try:
        subprocess.run(cmd_final, check=True, stderr=subprocess.DEVNULL)
        print(f"âœ… Merge Success! Final output: {os.path.basename(output_filename)}")
        
        # === ç§’æ•°ãƒã‚§ãƒƒã‚¯ ===
        orig_dur = get_video_duration(original_video_path)
        new_dur = get_video_duration(output_filename)
        
        print(f"   -----------------------------")
        print(f"   [Duration Check]")
        print(f"   Original: {orig_dur:.2f} sec")
        print(f"   Upscaled: {new_dur:.2f} sec")
        
        diff = abs(orig_dur - new_dur)
        if diff < 1.0:
            print("   âœ¨ PERFECT MATCH!")
        elif diff < 5.0:
            print("   âš ï¸ Slight difference (<5s). Usually OK.")
        else:
            print("   âŒ MAJOR LENGTH MISMATCH! Check log.")
        print(f"   -----------------------------")
            
    except:
        print("âŒ Merge failed.")

    if os.path.exists(list_txt): os.remove(list_txt)

def worker_process(video_path, workflow_file, start_frame, run_dir_name):
    try:
        start_frame = int(start_frame)
        cap = cv2.VideoCapture(video_path)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        cap.release()

        chunk_index = start_frame // CHUNK_SIZE
        
        # â˜…ã“ã“é‡è¦: Workerã¯ãƒãƒƒã‚·ãƒ¥ãƒ•ã‚©ãƒ«ãƒ€ã®ä¸­ã«ä¿å­˜ã™ã‚‹
        part_prefix = f"{run_dir_name}/part_{chunk_index:03d}"
        
        # ç”Ÿæˆæ¸ˆã¿ãƒã‚§ãƒƒã‚¯ç”¨ãƒ‘ã‚¹
        full_output_dir = os.path.join(COMFYUI_OUTPUT_DIR, run_dir_name)
        search_pattern = os.path.join(full_output_dir, f"part_{chunk_index:03d}*{OUTPUT_EXT}")
        existing = glob.glob(search_pattern)

        # æ—¢ã«ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã€ã‚µã‚¤ã‚ºãŒååˆ†ãªã‚‰ã‚¹ã‚­ãƒƒãƒ—
        if existing:
            if any(os.path.getsize(f) > 1024 for f in existing):
                print(f"[Worker] Chunk {chunk_index}: âœ… Exists in hash folder. Skipping.")
                sys.exit(0)
            else:
                 print(f"[Worker] Chunk {chunk_index}: âš ï¸ Found empty file, regenerating.")

        current_cap = min(CHUNK_SIZE, total_frames - start_frame)
        print(f"[Worker] Chunk {chunk_index}: Generating {current_cap} frames...")

        with open(workflow_file, "r", encoding="utf-8") as f:
            workflow = json.load(f)

        if NODE_ID_LOADER in workflow:
            workflow[NODE_ID_LOADER]["inputs"]["frame_load_cap"] = current_cap
            workflow[NODE_ID_LOADER]["inputs"]["skip_first_frames"] = start_frame
            workflow[NODE_ID_LOADER]["inputs"]["video"] = os.path.abspath(video_path)

        if NODE_ID_SAVER in workflow:
            workflow[NODE_ID_SAVER]["inputs"]["filename_prefix"] = part_prefix
            # â˜…30fpså¼·åˆ¶
            workflow[NODE_ID_SAVER]["inputs"]["frame_rate"] = TARGET_FPS 

        res = queue_prompt(workflow)
        if res and 'prompt_id' in res:
            wait_for_prompt_completion(res['prompt_id'])
            sys.exit(0)
        else:
            sys.exit(1)

    except Exception:
        traceback.print_exc()
        sys.exit(1)

def manager_process(original_video_path, workflow_file):
    print(f"=== Manager Started (Hash Isolation Mode) ===")
    cap = cv2.VideoCapture(original_video_path)
    if not cap.isOpened(): return
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    cap.release()
    
    # 1. ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰å®‰å…¨ãªåå‰ã‚’ä½œã‚‹ (SISTERS_åƒå¤01)
    base_name = os.path.splitext(os.path.basename(original_video_path))[0]
    safe_base_name = "".join([c if c.isalnum() or c in (' ', '.', '_', '-') else '_' for c in base_name])[:20]
    
    # 2. ãƒãƒƒã‚·ãƒ¥å€¤ã‚’è¨ˆç®— (æ··ã–ã‚Šé˜²æ­¢ãƒ»å†é–‹æ©Ÿèƒ½ã®æ ¸)
    # ãƒ•ã‚¡ã‚¤ãƒ«åãŒåŒã˜ãªã‚‰ã€ã„ã¤å®Ÿè¡Œã—ã¦ã‚‚åŒã˜ãƒãƒƒã‚·ãƒ¥ã«ãªã‚‹ -> ç¢ºå®Ÿã«å†é–‹ã§ãã‚‹
    filename_hash = hashlib.md5(base_name.encode('utf-8')).hexdigest()[:8]
    
    # 3. ä½œæ¥­ç”¨ãƒ•ã‚©ãƒ«ãƒ€å: åå‰_ãƒãƒƒã‚·ãƒ¥ (ä¾‹: SISTERS_a1b2c3d4)
    run_dir_name = f"{safe_base_name}_{filename_hash}"
    target_dir_path = os.path.join(COMFYUI_OUTPUT_DIR, run_dir_name)
    
    # ãƒ•ã‚©ãƒ«ãƒ€ç¢ºèªãƒ»ä½œæˆ
    if os.path.exists(target_dir_path):
        print(f"ğŸ”„ Resuming hash folder: {run_dir_name}")
    else:
        os.makedirs(target_dir_path, exist_ok=True)
        print(f"ğŸ†• Created unique work folder: {run_dir_name}")

    tasks = []
    for i in range(0, total_frames, CHUNK_SIZE):
        tasks.append(i)

    running_procs = [] 
    task_iter = iter(tasks)
    error_occurred = False

    while True:
        for p, frame in running_procs[:]:
            if p.poll() is not None:
                if p.returncode != 0:
                    error_occurred = True
                    break
                running_procs.remove((p, frame))
        
        if error_occurred: break

        while len(running_procs) < MAX_PARALLEL_WORKERS:
            try:
                next_start_frame = next(task_iter)
                chunk_index = next_start_frame // CHUNK_SIZE
                
                # ä½œæ¥­ãƒ•ã‚©ãƒ«ãƒ€ã®ä¸­ã«ã€è©²å½“ãƒ‘ãƒ¼ãƒ„ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                search_pattern = os.path.join(target_dir_path, f"part_{chunk_index:03d}*{OUTPUT_EXT}")
                if glob.glob(search_pattern):
                    # ã™ã§ã«ã‚ã‚Œã°ã‚¹ã‚­ãƒƒãƒ— (ã“ã“ãŒå†é–‹æ©Ÿèƒ½)
                    continue

                cmd = [sys.executable, __file__, original_video_path, workflow_file, 
                       "--worker_mode", 
                       "--start_frame", str(next_start_frame), 
                       "--run_id", run_dir_name] # Workerã«ã¯ä½œæ¥­ãƒ•ã‚©ãƒ«ãƒ€åã‚’æ¸¡ã™
                proc = subprocess.Popen(